{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95b88b3-f7c1-48cc-ac9e-619ca5f5f4a8",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #492c68;\">NLP: Sentiment Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0feb6-93b0-4616-a894-50ecfc8f6371",
   "metadata": {},
   "source": [
    "We are going dive into a basic introduction for <mark>Sentiment Analisis</mark> through <mark>Natural Language Processing</mark>. \n",
    "\n",
    "- As always, we read our dataset. Let's see what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1312fb19-bb7e-4b3a-9747-7a3af8f48cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## EDA Libraries\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import plotly as px\n",
    "\n",
    "## Settings configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75957d1-c6be-4440-8b78-0cfbfa9ef1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jesus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\jesus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jesus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\jesus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\jesus\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reviews.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870d1e0-17e0-4845-829f-527ffa145486",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best setting for columns names\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(\" \",\"_\").str.replace(\".\",\"_\").str.replace(\":\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc4fb7-df38-4b82-bd89-c2858a996699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98afbf-e4df-4f9c-9f1f-2836f374bb71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b6bb8-0640-4d37-abe9-e7df9ebbe479",
   "metadata": {},
   "source": [
    "From df we have interest about <mark>\"score\"</mark>, <mark>\"summary\"</mark> and <mark>\"text\"</mark>\n",
    "\n",
    "- \"text\": is the most important variable in our project. These texts will be break down into tokens and processed to figure out their sentiment.\n",
    "- \"score\" and \"sumary\" are useful to compare our new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f301ef-09b5-49cf-873d-91d20b405da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[[\"text\", \"score\", \"summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e943cb1-a17b-4841-8fc7-abe3856a7ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d1856-f7db-48f6-8a8b-415f037e7cc8",
   "metadata": {},
   "source": [
    "- Before we go further, let's plot \"score\" to see its distribution and have an idea of the data we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35ea59-c61d-49c1-bf27-f47396fd546e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.barplot(data= df[\"score\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2d8ac-2bb2-495d-9fde-94507a6086c1",
   "metadata": {},
   "source": [
    "- Lenght its too big. It would be better if we use a fraction of it to make it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ba73e-8aba-4153-a61f-618082a230fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6cfe3-8df5-4713-b0dc-83592d33c6b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2 style=\"color: #327a81;\">NLP Preprocessing: NLTK Basics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d20420-ee61-40a1-81d3-355c56d76e4b",
   "metadata": {},
   "source": [
    "<mark>NLTK</mark> means <mark>Natural Language Toolkit</mark>. It's a basic tool for <mark>NLP beginners</mark> and <mark>quick model processing</mark>. It's not too fancy but can be used to understand basics.\n",
    "\n",
    "- Let's see how NLTK works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee623e9-6b03-4420-9304-383c48685bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Installing the specific librarie\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt') #tokenizer package\n",
    "nltk.download('averaged_perceptron_tagger') #For label tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec68b5-5517-4797-b29c-8f46819882b7",
   "metadata": {},
   "source": [
    "- For this demo. We take a random index as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e74adc-00b4-424f-8699-ff4c965ef101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = df[\"text\"][65]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26c5db-d2cf-4a2c-98ac-61a269fcae47",
   "metadata": {},
   "source": [
    "- NTLK can tokenize any text. This is crucial for the ML preprocessing.\n",
    "- Tokenizer may seem similar to splitting strings, but it's more complex than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddeffb-5aac-4cca-90f3-079123189135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(example) # word_tokenize breaks the string into tokens that the machine understands\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a168214-d096-4e13-8d08-599782eb014e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(tokens) # pos_tag shows the grammar labels of the tokens. Look for pos_tag list in google to know more about abreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dccaee-3cfa-4bb0-aaf8-73b11c29f9b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2 style=\"color: #327a81;\">Sentiment Analysis: VADER and RoBERTa</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d70b8-ad54-45a8-b217-6903b9f31702",
   "metadata": {},
   "source": [
    "There are lot of ways to do Sentiment Analaysis. For this introduction we will see two different types.\n",
    "\n",
    "- <mark>VADER</mark> to see the <mark>positive/negative valance</mark> of the sentences (basic)\n",
    "- <mark>RoBERTa</mark> to do a <mark>mood analysis</mark> (complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d83b83-13f0-4df4-ada1-2c07e8c98fb5",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #60b671;\">VADER</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b770aa-801a-41c3-829c-ae5e91309d04",
   "metadata": {},
   "source": [
    "VADER stands for <mark>Valance Aware Dictionary and Sentiment Reasoner</mark>. This NLTK tool is used to figure out if a piece of text is expressing positive, negative or neutral emotions.\n",
    "\n",
    "- This method uses a <mark>\"bag of words\"</mark> approach. This means that the analysis uses the weight of the words (understands the positive charge without context)\n",
    "- Can be <mark>more accessible for beginners</mark> than RoBERTa. It's recommended for fast prototypes... but has lots of limitations (self-awareness, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c12e1-73f8-457d-94d5-73f32ce8c6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Installing NLTK specific function\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf541ed6-4bb7-4acc-a08a-8c872fecf6cb",
   "metadata": {},
   "source": [
    "- Once we have defined our sia, we can test it with our previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ecbdd-5a37-47f8-9999-6ac92fdeaa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b2369-7f68-4de4-a00f-4cc9075addda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia.polarity_scores(example) # polarity_scores give us pos/neg/neu values from -1 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b56ca8-741d-4a9b-aa15-0ca2bde6d608",
   "metadata": {},
   "source": [
    "- Scores in the example are not bad. Tends to be neutral/positive, ditching the negative value\n",
    "- So this matches with the real score and its summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a76227-8d77-4a29-bffc-f89ad0cee40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[65] # Check the example's polarity with real ratings, to see if it fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08601cf9-606d-49f1-96f1-d96386140345",
   "metadata": {},
   "source": [
    "Now that we know how polarity scores works, let's apply this function to all texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c7186-1edc-45d6-9e82-89c388adb988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Polarity Scores through Iterrows on the \"text\" column\n",
    "\n",
    "pol_scores = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    scores = sia.polarity_scores(text)\n",
    "    pol_scores[i] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de47377-4bbd-4504-afde-35a3829009ce",
   "metadata": {},
   "source": [
    "- We can compare the polarity scores with \"score\" and \"summary\" to check if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bde29-d673-4f04-990b-09f492dd1289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pol_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2bc718-a2ad-43bd-a688-1f5805011d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's unify both dataframes (original + polarity scores) to see if our sentiments match\n",
    "\n",
    "vaders = pd.DataFrame(pol_scores).T\n",
    "\n",
    "vaders = pd.concat([df, vaders], axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464adbe-06d4-447d-9da5-ea2654e97578",
   "metadata": {},
   "source": [
    "- Let's check how scores 1 went after the polarity. It would tend to negative side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6776ef-249a-43c0-8ee2-c86b55ad16e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vaders.groupby(\"score\").get_group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401a4e7-ded2-4391-b4cf-214436c19a48",
   "metadata": {
    "tags": []
   },
   "source": [
    "- We can compare ratings with polarity scores through a quick plot.\n",
    "- We can use \"compound\" to see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcce5fc-46b7-4067-966a-da20a6447af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=vaders, x=\"score\", y=\"compound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7deda-3e50-4029-89ac-7bb3225d1ad2",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #60b671;\">RoBERTa</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9afc2-bf89-46fb-9e8c-57726828337e",
   "metadata": {},
   "source": [
    "RoBERTa stands for <mark>Robustly Optimized BERT Approach</mark>. Is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model. \n",
    "\n",
    "- Transformers is a step fordward in the NLP field. This deep learning model focus on <mark>attention mechanism</mark>, which allows it to recognise different parts of an input sequence.\n",
    "- The attention mechanism is key to a text's <mark>self-awareness</mark>. Complex syntactic structures or different intentions, like sarcasm, can be processed by Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46730c13-87ca-4431-bab6-65bce21147a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Installing Transformers specific functions\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2066042-2839-4c77-a359-38a3b9ee4055",
   "metadata": {},
   "source": [
    "- We will use a RoBERTa petrained model named Emotion English DistilRoBERTa-base. It's a refined embedding that can <mark>break text into emotion moods</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3a771-f49b-45a7-a05f-f2b40f16c5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "\n",
    "MODEL = f\"j-hartmann/emotion-english-distilroberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301ab2f-013d-4331-b073-219be57b0e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see through a pipeline example how it works\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", MODEL, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bf865-5a2f-4b7c-b759-eca42c818aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try any example message\n",
    "\n",
    "classifier(\"It was really nice to meet you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525df7bc-c8e5-471b-a4e1-696bfa9c15fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "- After seeing how transformers works, let's apply the model in our texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c1c37-bd6b-491f-9816-5fdfd69ad5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save functions that will help us\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007f5c2-fafd-42f8-835e-33c938a1b811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function for obtaining the scores\n",
    "\n",
    "def mooder(text):\n",
    "    max_lenght = 512\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\", max_length=max_lenght, truncation=True, padding=\"longest\")\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    moods = {\n",
    "    \"anger\": scores[0],\n",
    "    \"disgust\" : scores[1],\n",
    "    \"fear\": scores[2],\n",
    "    \"joy\": scores[3],\n",
    "    \"neutral\": scores[4], \n",
    "    \"sadness\": scores[5],\n",
    "    \"surprise\": scores[6]\n",
    "    }\n",
    "    \n",
    "    return moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe5326-d7c6-443b-a67d-6389f0744f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roberta =  df[\"text\"].apply(mooder) # We apply the function defined before to have all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40082692-13cd-4faf-9dda-521fa5ae2de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roberta = pd.DataFrame(roberta)\n",
    "roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58940469-570c-44a3-8c4c-1d69391af9be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roberta_scores = pd.json_normalize(roberta[\"text\"]) # Normalize the df to put all elements in dictionary as columns and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0f7ce-2eeb-4368-8666-00f7b023ba1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moods = pd.concat([df, roberta_scores], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40672e6-12d0-49ad-a40e-e31dccb54fff",
   "metadata": {},
   "source": [
    "- Let's take a sneak peak to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b75f91-e5be-45f8-996f-65e8f7aed69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moods.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ea136-ada7-4ef1-aa13-f02fa7b48571",
   "metadata": {},
   "source": [
    "- We could check by any mood the scores to see if matches our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1925c23-ad1c-4f00-a9a7-7ba9b636450b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moods.sort_values(by=\"disgust\", ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160713f2-0bd7-465c-89d5-9bb67bc7de37",
   "metadata": {
    "tags": []
   },
   "source": [
    "- As we did it before with VADERS, we can plot and compare ratings with a specific type of score to see if it fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843374c-8f46-465b-b723-39199e493362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=moods, x=\"score\", y=\"disgust\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
